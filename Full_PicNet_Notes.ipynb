{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import random\n",
    "import imageio as io\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Network\n",
    "This is the convolutional network that will learn to classify eruptions from the IR pictures of the lava lake on Mt. Erebus. It is written in Pytorch and modeled after the VGG net. It consist of 6 convolutional layers with a max pooling layer after every other conv. layer. These layers pick out features in the pictures used to classify the images. After that there are 3 linear, fully connected layers used to actually classify the images. Totalling 9 layers in all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class piccnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(piccnn, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 34, 3)\n",
    "        self.conv2 = nn.Conv2d(34, 34, 3)\n",
    "        self.conv3 = nn.Conv2d(34, 64, 3)\n",
    "        self.conv4 = nn.Conv2d(64, 64, 3)\n",
    "        self.conv5 = nn.Conv2d(64, 128, 3)\n",
    "        self.conv6 = nn.Conv2d(128, 128, 3)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(3, 3)\n",
    "\n",
    "        self.fc1 = nn.Linear(40320, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 1000)\n",
    "        self.fc3 = nn.Linear(1000, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = self.pool(F.relu(self.conv6(x)))\n",
    "        x = x.view(-1, self.numflatfeatures(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "    def numflatfeatures(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class\n",
    "This is a class that will contain all the information necessary to manipulate the data or images. It allows us to get the total size of the dataset and pull small batches of images with labels for training or validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, epaths, npaths, data_folder):\n",
    "        '''\n",
    "        This is the dataset class; it holds file paths for the data and retrieves the photos as they \n",
    "        are needed, outputting them as numpy arrays to be used in training run\n",
    "        epaths: {numpy array}; array of file names for events in data folder\n",
    "        npaths: {numpy array}; array of file names for nonevents in data folder\n",
    "        data_folder: {String}; dataset directory or folder\n",
    "        '''\n",
    "        self.epaths = epaths\n",
    "        self.npaths = npaths\n",
    "        self.folder = data_folder\n",
    "        \n",
    "    def getSize(self):\n",
    "        '''\n",
    "        Returns the total size of the dataset\n",
    "        '''\n",
    "        return len(self.epaths) + len(self.npaths)\n",
    "        \n",
    "    def getRandomBatchTrain(self, batch_size):\n",
    "        '''\n",
    "        Returns a randomized batch of data and its labels are structured for BCEWithLogitsLoss\n",
    "        batch_size: {Integer}; this is the number of photos to pull from the dataset\n",
    "        '''\n",
    "        np.random.shuffle(self.epaths)\n",
    "        np.random.shuffle(self.npaths)\n",
    "        batch = list()\n",
    "        labels = list()\n",
    "        it = int(batch_size / 2)\n",
    "        for i in range(it):\n",
    "            image1 = io.imread(os.path.join(self.folder, self.epaths[i]))\n",
    "            image2 = io.imread(os.path.join(self.folder, self.npaths[i]))\n",
    "            batch.append(image1)\n",
    "            labels.append([0, 1])\n",
    "            batch.append(image2)\n",
    "            labels.append([1, 0])\n",
    "            \n",
    "        if len(batch) > batch_size:\n",
    "            batch = batch[:batch_size]\n",
    "            \n",
    "        return np.array(batch), np.array(labels)\n",
    "    \n",
    "    def getRandomBatchVal(self, batch_size):\n",
    "        '''\n",
    "        Returns a randomized batch of data and its labels\n",
    "        batch_size: {Integer}; this is the number of photos to pull from the dataset\n",
    "        '''\n",
    "        np.random.shuffle(self.epaths)\n",
    "        np.random.shuffle(self.npaths)\n",
    "        batch = list()\n",
    "        labels = list()\n",
    "        it = int(batch_size / 2)\n",
    "        for i in range(it):\n",
    "            image1 = io.imread(os.path.join(self.folder, self.epaths[i]))\n",
    "            image2 = io.imread(os.path.join(self.folder, self.npaths[i]))\n",
    "            batch.append(image1)\n",
    "            labels.append(1)\n",
    "            batch.append(image2)\n",
    "            labels.append(0)\n",
    "            \n",
    "        if len(batch) > batch_size:\n",
    "            batch = batch[:batch_size]\n",
    "            \n",
    "        return np.array(batch), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Run\n",
    "This function is used to train the network. The function loops over the data for the specified number of times, pulling a minibatch of specified size, norms the images, feeds it into the network, then depending on performance updates the networks weights. It provides updates on the training by outputting the total loss after each epoch. After the training is finished it gives the total time taken for the function to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_run(network, dataset, criterion, optimizer, batch_size,\n",
    "                 learning_rate, scheduler, num_epochs=200):\n",
    "    '''\n",
    "    This is the function that will train the network.\n",
    "    network: {pytorch network}; the piccnn above\n",
    "    criterion: {pytorch criterion fucntion}; this is the loss function\n",
    "    optimizer: {pytorch optimizer function}; how network is updated; I use standard gradient descent (most common)\n",
    "    batch_size: {Integer}; number of pics to pull for training batch usually set to 125/250; need to create dataset first\n",
    "    learning_rate: {Float}; how quickly the network is trained\n",
    "    scheduler: {pytorch scheduler function}; updates learning rate if the network stalls in its learning (shouldn't have to mess with this)\n",
    "    num_epochs: {Integer}; number of times the dataset it run through.\n",
    "    '''\n",
    "    timeit = time.time()\n",
    "    norm = nn.BatchNorm2d(1)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-'*10)\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        \n",
    "        num_batches = int(dataset.getSize() / batch_size) - 1\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            data, labels = dataset.getRandomBatchTrain(batch_size)\n",
    "            data = torch.from_numpy(data).float()\n",
    "            labels = torch.tensor(labels)\n",
    "#             data = norm(data.view(batch_size, 1, 480, 640))\n",
    "            data = norm(torch.unsqueeze(data, 1))\n",
    "            data = Variable(data).float().to(device)\n",
    "            labels = Variable(labels).float().to(device)\n",
    "            \n",
    "            # Start of Training\n",
    "            optimizer.zero_grad()\n",
    "            output = network(data)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update the weights\n",
    "            for f in network.parameters():\n",
    "                f.data -= (f.grad.data * learning_rate)\n",
    "                \n",
    "            running_loss += loss.item() * data.size(0)\n",
    "            \n",
    "        epoch_loss = running_loss / dataset.getSize()\n",
    "        scheduler.step(epoch_loss)\n",
    "        \n",
    "        print('Loss: {:.4f}'.format(epoch_loss))\n",
    "        print()\n",
    "        \n",
    "    total_time = time.time() - timeit\n",
    "    print('Training done in {:.0f}m {:.0f}s'.format(total_time // 60, total_time % 60))\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Function\n",
    "This does much the same thing as the training_run function. The difference is that it does not update the network and it also keeps track of the accuracy of the network on the new dataset. This accuracy is then output after the validation data has been looped through along with the total run time of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_run(net, dataset, batch_size=10):\n",
    "    timeit = time.time()\n",
    "    norm = nn.BatchNorm2d(1)\n",
    "    correct = 0\n",
    "    wrong = 0\n",
    "        \n",
    "    num_batches = int(dataset.getSize() / batch_size) - 1\n",
    "        \n",
    "    for i in range(num_batches):\n",
    "        data, labels = dataset.getRandomBatchVal(batch_size)\n",
    "        data = torch.from_numpy(data).float()\n",
    "        data = norm(torch.unsqueeze(data, 1))\n",
    "        data = Variable(data).float().to(device)\n",
    "            \n",
    "        output = net(data)\n",
    "        _, preds = torch.max(output, 1)\n",
    "        preds = preds.data.cpu().numpy()\n",
    "        for i in range(batch_size):\n",
    "            if preds[i] == labels[i]:\n",
    "                correct += 1\n",
    "            else:\n",
    "                wrong += 1\n",
    "                \n",
    "    grade = correct / (correct+wrong)\n",
    "    print('Accuracy: ', grade*100)\n",
    "    print('Correct: ', correct)\n",
    "    print('Missed: ', wrong)\n",
    "    print()\n",
    "    total_time = time.time() - timeit\n",
    "    print('Training done in {:.0f}m {:.0f}s'.format(total_time // 60, total_time % 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep for Training\n",
    "Here the training dataset object is created, the network is initalized, and the functions and variables used in the training run are created. The optimizer is the Standard Gradient Descent, the scheduler simply reduces the learning_rate when the network's loss after each epoch reaches a plateau, and the criterion is BCEwithLogitsLoss as explained at https://pytorch.org/docs/stable/nn.html#bcewithlogitsloss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = 'dataset'\n",
    "events = np.genfromtxt('event_files.csv', delimiter=',', dtype=str)\n",
    "nonevents = np.genfromtxt('nonevents.csv', delimiter=',', dtype=str)\n",
    "\n",
    "dataset = Dataset(events, nonevents, home_dir)\n",
    "net = piccnn()\n",
    "net.to(device)\n",
    "\n",
    "learning_rate = 0.001\n",
    "batchSize = 15\n",
    "\n",
    "lenneg = len(nonevents)\n",
    "lenpos = len(events)\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=.1, verbose=True)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([lenneg/(lenpos+lenneg), lenpos/(lenpos+lenneg)]).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function training_run is called and the network is trained on the dataset intialized above. Progress is output after each epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/55\n",
      "----------\n",
      "Loss: 0.4227\n",
      "\n",
      "Epoch 2/55\n",
      "----------\n",
      "Loss: 0.4214\n",
      "\n",
      "Epoch 3/55\n",
      "----------\n",
      "Loss: 0.4199\n",
      "\n",
      "Epoch 4/55\n",
      "----------\n",
      "Loss: 0.4183\n",
      "\n",
      "Epoch 5/55\n",
      "----------\n",
      "Loss: 0.4168\n",
      "\n",
      "Epoch 6/55\n",
      "----------\n",
      "Loss: 0.4153\n",
      "\n",
      "Epoch 7/55\n",
      "----------\n",
      "Loss: 0.4139\n",
      "\n",
      "Epoch 8/55\n",
      "----------\n",
      "Loss: 0.4125\n",
      "\n",
      "Epoch 9/55\n",
      "----------\n",
      "Loss: 0.4112\n",
      "\n",
      "Epoch 10/55\n",
      "----------\n",
      "Loss: 0.4098\n",
      "\n",
      "Epoch 11/55\n",
      "----------\n",
      "Loss: 0.4085\n",
      "\n",
      "Epoch 12/55\n",
      "----------\n",
      "Loss: 0.4073\n",
      "\n",
      "Epoch 13/55\n",
      "----------\n",
      "Loss: 0.4060\n",
      "\n",
      "Epoch 14/55\n",
      "----------\n",
      "Loss: 0.4048\n",
      "\n",
      "Epoch 15/55\n",
      "----------\n",
      "Loss: 0.4036\n",
      "\n",
      "Epoch 16/55\n",
      "----------\n",
      "Loss: 0.4024\n",
      "\n",
      "Epoch 17/55\n",
      "----------\n",
      "Loss: 0.4012\n",
      "\n",
      "Epoch 18/55\n",
      "----------\n",
      "Loss: 0.4001\n",
      "\n",
      "Epoch 19/55\n",
      "----------\n",
      "Loss: 0.3990\n",
      "\n",
      "Epoch 20/55\n",
      "----------\n",
      "Loss: 0.3979\n",
      "\n",
      "Epoch 21/55\n",
      "----------\n",
      "Loss: 0.3968\n",
      "\n",
      "Epoch 22/55\n",
      "----------\n",
      "Loss: 0.3957\n",
      "\n",
      "Epoch 23/55\n",
      "----------\n",
      "Loss: 0.3946\n",
      "\n",
      "Epoch 24/55\n",
      "----------\n",
      "Loss: 0.3933\n",
      "\n",
      "Epoch 25/55\n",
      "----------\n",
      "Loss: 0.3923\n",
      "\n",
      "Epoch 26/55\n",
      "----------\n",
      "Loss: 0.3910\n",
      "\n",
      "Epoch 27/55\n",
      "----------\n",
      "Loss: 0.3901\n",
      "\n",
      "Epoch 28/55\n",
      "----------\n",
      "Loss: 0.3890\n",
      "\n",
      "Epoch 29/55\n",
      "----------\n",
      "Loss: 0.3876\n",
      "\n",
      "Epoch 30/55\n",
      "----------\n",
      "Loss: 0.3862\n",
      "\n",
      "Epoch 31/55\n",
      "----------\n",
      "Loss: 0.3850\n",
      "\n",
      "Epoch 32/55\n",
      "----------\n",
      "Loss: 0.3837\n",
      "\n",
      "Epoch 33/55\n",
      "----------\n",
      "Loss: 0.3821\n",
      "\n",
      "Epoch 34/55\n",
      "----------\n",
      "Loss: 0.3811\n",
      "\n",
      "Epoch 35/55\n",
      "----------\n",
      "Loss: 0.3789\n",
      "\n",
      "Epoch 36/55\n",
      "----------\n",
      "Loss: 0.3768\n",
      "\n",
      "Epoch 37/55\n",
      "----------\n",
      "Loss: 0.3757\n",
      "\n",
      "Epoch 38/55\n",
      "----------\n",
      "Loss: 0.3735\n",
      "\n",
      "Epoch 39/55\n",
      "----------\n",
      "Loss: 0.3693\n",
      "\n",
      "Epoch 40/55\n",
      "----------\n",
      "Loss: 0.3659\n",
      "\n",
      "Epoch 41/55\n",
      "----------\n",
      "Loss: 0.3618\n",
      "\n",
      "Epoch 42/55\n",
      "----------\n",
      "Loss: 0.3564\n",
      "\n",
      "Epoch 43/55\n",
      "----------\n",
      "Loss: 0.3488\n",
      "\n",
      "Epoch 44/55\n",
      "----------\n",
      "Loss: 0.3369\n",
      "\n",
      "Epoch 45/55\n",
      "----------\n",
      "Loss: 0.3218\n",
      "\n",
      "Epoch 46/55\n",
      "----------\n",
      "Loss: 0.2954\n",
      "\n",
      "Epoch 47/55\n",
      "----------\n",
      "Loss: 0.2251\n",
      "\n",
      "Epoch 48/55\n",
      "----------\n",
      "Loss: 0.1921\n",
      "\n",
      "Epoch 49/55\n",
      "----------\n",
      "Loss: 0.1238\n",
      "\n",
      "Epoch 50/55\n",
      "----------\n",
      "Loss: 0.1384\n",
      "\n",
      "Epoch 51/55\n",
      "----------\n",
      "Loss: 0.0794\n",
      "\n",
      "Epoch 52/55\n",
      "----------\n",
      "Loss: 0.1430\n",
      "\n",
      "Epoch 53/55\n",
      "----------\n",
      "Loss: 0.1014\n",
      "\n",
      "Epoch 54/55\n",
      "----------\n",
      "Loss: 0.0643\n",
      "\n",
      "Epoch 55/55\n",
      "----------\n",
      "Loss: 0.0341\n",
      "\n",
      "Training done in 3m 37s\n"
     ]
    }
   ],
   "source": [
    "net = training_run(net, dataset, criterion, optimizer, batchSize, learning_rate, scheduler, num_epochs=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep for Validation\n",
    "The validation dataset it initalized. This is a completely new dataset where the network has not seen any of the images it contains. This new dataset is used to make sure the network actually learned some. After the validation dataset is loaded the network is put into evaluation mode and we are ready to test if it learned anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piccnn(\n",
       "  (conv1): Conv2d(1, 34, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv3): Conv2d(34, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=40320, out_features=1000, bias=True)\n",
       "  (fc2): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "  (fc3): Linear(in_features=1000, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_dir = 'validation_dataset'\n",
    "event_paths = np.genfromtxt('validation_events.csv', delimiter=',', dtype=str)\n",
    "nonevent_paths = np.genfromtxt('validation_nonevents.csv', delimiter=',', dtype=str)\n",
    "\n",
    "val_data = Dataset_Validation(event_paths, nonevent_paths, home_dir)\n",
    "\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function validation_run is called and the networks ability to detect eruptions based on the IR images is put to the test. At the end the accuracy as well as the number of correctly and incorrectly labeled images is printed out along with the total time for the function to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9514285714285714\n",
      "Correct:  333\n",
      "Missed:  17\n"
     ]
    }
   ],
   "source": [
    "validation_run(net, val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
